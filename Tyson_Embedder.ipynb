{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a55143e-e607-4dde-9843-ee953a88e642",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pinecone sentence-transformers boto3 tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07eb4368-382c-40e7-a506-4d906de834fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import logging\n",
    "import re\n",
    "\n",
    "# Set up logging to both console and S3\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Function to parse S3 URI\n",
    "def parse_s3_uri(s3_uri):\n",
    "    \"\"\"\n",
    "    Parse an S3 URI to extract bucket name and prefix\n",
    "    \n",
    "    Args:\n",
    "        s3_uri (str): S3 URI in the format s3://bucket-name/prefix/path/\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (bucket_name, prefix)\n",
    "    \"\"\"\n",
    "    pattern = r's3://([^/]+)/?(.*)'\n",
    "    match = re.match(pattern, s3_uri)\n",
    "    if match:\n",
    "        bucket_name = match.group(1)\n",
    "        prefix = match.group(2)\n",
    "        return bucket_name, prefix\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid S3 URI format: {s3_uri}\")\n",
    "\n",
    "# Custom S3 log handler class\n",
    "class S3LogHandler(logging.Handler):\n",
    "    def __init__(self, s3_uri, interval=60):\n",
    "        \"\"\"\n",
    "        Initialize S3 log handler that uploads logs to S3 at regular intervals\n",
    "        \n",
    "        Args:\n",
    "            s3_uri (str): S3 URI for log storage\n",
    "            interval (int): Upload interval in seconds\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Parse S3 URI to get bucket and prefix\n",
    "        self.bucket_name, self.log_prefix = parse_s3_uri(s3_uri)\n",
    "        \n",
    "        # Make sure prefix ends with a slash\n",
    "        if self.log_prefix and not self.log_prefix.endswith('/'):\n",
    "            self.log_prefix += '/'\n",
    "            \n",
    "        self.interval = interval\n",
    "        self.buffer = []\n",
    "        self.last_upload_time = time.time()\n",
    "        \n",
    "        # Create S3 client\n",
    "        self.s3_client = boto3.client('s3')\n",
    "        \n",
    "        # Set formatter\n",
    "        self.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))\n",
    "        \n",
    "        # Create a unique log file name with timestamp\n",
    "        timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "        self.log_filename = f\"vector_embedding_process_{timestamp}.log\"\n",
    "        \n",
    "        logger.info(f\"S3 logging enabled. Logs will be uploaded to s3://{self.bucket_name}/{self.log_prefix}{self.log_filename}\")\n",
    "    \n",
    "    def emit(self, record):\n",
    "        \"\"\"Process a log record by adding it to buffer and uploading if needed\"\"\"\n",
    "        log_entry = self.format(record)\n",
    "        self.buffer.append(log_entry)\n",
    "        \n",
    "        # Upload logs if interval has passed\n",
    "        current_time = time.time()\n",
    "        if current_time - self.last_upload_time >= self.interval:\n",
    "            self.upload_logs()\n",
    "    \n",
    "    def upload_logs(self):\n",
    "        \"\"\"Upload accumulated logs to S3\"\"\"\n",
    "        if not self.buffer:\n",
    "            return\n",
    "            \n",
    "        log_content = \"\\n\".join(self.buffer)\n",
    "        s3_key = f\"{self.log_prefix}{self.log_filename}\"\n",
    "        \n",
    "        try:\n",
    "            self.s3_client.put_object(\n",
    "                Bucket=self.bucket_name,\n",
    "                Key=s3_key,\n",
    "                Body=log_content\n",
    "            )\n",
    "            self.last_upload_time = time.time()\n",
    "            print(f\"Logs uploaded to s3://{self.bucket_name}/{s3_key}\")\n",
    "            # Don't clear buffer as we want to keep the full log history in each upload\n",
    "            # (appending approach)\n",
    "        except Exception as e:\n",
    "            # Log to console since we can't use logger here (would cause recursion)\n",
    "            print(f\"Error uploading logs to S3: {e}\")\n",
    "    \n",
    "    def flush(self):\n",
    "        \"\"\"Upload any remaining logs when handler is closed\"\"\"\n",
    "        self.upload_logs()\n",
    "        self.buffer = []\n",
    "\n",
    "# Function to set up S3 logging\n",
    "def setup_s3_logging(s3_uri, interval=60):\n",
    "    \"\"\"\n",
    "    Set up logging to S3\n",
    "    \n",
    "    Args:\n",
    "        s3_uri (str): S3 URI for log files in format s3://bucket/prefix\n",
    "        interval (int): Interval in seconds to upload logs to S3\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create and add the S3 handler\n",
    "        s3_handler = S3LogHandler(s3_uri, interval)\n",
    "        logger.addHandler(s3_handler)\n",
    "        \n",
    "        return s3_handler\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to set up S3 logging: {e}\")\n",
    "        return None\n",
    "\n",
    "# Environment variables \n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = \"AKIA47CRW2KZIK6G4JDN\"\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"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\"\n",
    "os.environ[\"AWS_REGION\"] = \"us-east-2\"  \n",
    "os.environ[\"PINECONE_API_KEY\"] = \"pcsk_3rWgJE_FzWZNr3H2bxL5nyTHJgkMS16qhJWEBXWupJuFm8M5gUKTGittSVeG2VLSFfH2RR\"\n",
    "os.environ[\"S3_LOG_URI\"] = \"s3://tyson-chatbot-pipeline-storage/logs/Embedding_logs/\"  \n",
    "\n",
    "# Initialize DynamoDB client\n",
    "def init_dynamodb():\n",
    "    try:\n",
    "        dynamodb = boto3.resource('dynamodb')\n",
    "        return dynamodb\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to initialize DynamoDB: {e}\")\n",
    "        raise\n",
    "\n",
    "# Load embedding model\n",
    "def load_embedding_model():\n",
    "    try:\n",
    "        # Load the all-MiniLM-L6-v2 model (outputs 384-dimensional embeddings)\n",
    "        model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load embedding model: {e}\")\n",
    "        raise\n",
    "\n",
    "# Initialize Pinecone\n",
    "def init_pinecone(index_name=\"neil-degrasse-tyson-embeddings\"):\n",
    "    try:\n",
    "        # Initialize Pinecone client (v3 format for serverless)\n",
    "        from pinecone import Pinecone, ServerlessSpec\n",
    "        \n",
    "        pc = Pinecone(api_key=os.environ[\"PINECONE_API_KEY\"])\n",
    "        \n",
    "        # Check if the index exists\n",
    "        indexes = pc.list_indexes()\n",
    "        index_exists = any(index.name == index_name for index in indexes)\n",
    "        \n",
    "        # If index doesn't exist, create it\n",
    "        if not index_exists:\n",
    "            # Create a serverless index\n",
    "            index_config = pc.create_index(\n",
    "                name=index_name,\n",
    "                dimension=384,  # all-MiniLM-L6-v2 produces 384-dimensional vectors\n",
    "                metric=\"cosine\",\n",
    "                spec=ServerlessSpec(\n",
    "                    cloud=\"aws\",\n",
    "                    region=\"us-east-1\"  # You can change this to your preferred region\n",
    "                )\n",
    "            )\n",
    "            logger.info(f\"Created new Pinecone serverless index: {index_name}\")\n",
    "            # Wait for index to be ready\n",
    "            while not pc.describe_index(index_name).status.ready:\n",
    "                logger.info(\"Waiting for index to be ready...\")\n",
    "                time.sleep(5)\n",
    "        else:\n",
    "            index_config = pc.describe_index(index_name)\n",
    "            logger.info(f\"Using existing Pinecone index: {index_name}\")\n",
    "        \n",
    "        # Connect to the index\n",
    "        index = pc.Index(host=index_config.host)\n",
    "        return index\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to initialize Pinecone: {e}\")\n",
    "        raise\n",
    "\n",
    "# Scan DynamoDB table for all items\n",
    "def get_all_chunks(dynamodb, table_name=\"ChunkDB\"):\n",
    "    try:\n",
    "        table = dynamodb.Table(table_name)\n",
    "        response = table.scan()\n",
    "        items = response['Items']\n",
    "        \n",
    "        # Handle pagination if there are more items\n",
    "        while 'LastEvaluatedKey' in response:\n",
    "            response = table.scan(ExclusiveStartKey=response['LastEvaluatedKey'])\n",
    "            items.extend(response['Items'])\n",
    "        \n",
    "        logger.info(f\"Retrieved {len(items)} chunks from DynamoDB\")\n",
    "        return items\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to retrieve chunks from DynamoDB: {e}\")\n",
    "        raise\n",
    "\n",
    "# Create embeddings for chunks and upload to Pinecone\n",
    "def process_chunks_to_pinecone(chunks, model, index, batch_size=100):\n",
    "    try:\n",
    "        total_chunks = len(chunks)\n",
    "        total_batches = (total_chunks + batch_size - 1) // batch_size\n",
    "        \n",
    "        for batch_idx in tqdm(range(total_batches), desc=\"Processing batches\"):\n",
    "            start_idx = batch_idx * batch_size\n",
    "            end_idx = min((batch_idx + 1) * batch_size, total_chunks)\n",
    "            \n",
    "            batch_chunks = chunks[start_idx:end_idx]\n",
    "            \n",
    "            # Extract text from chunks\n",
    "            texts = [chunk.get('text', '') for chunk in batch_chunks]\n",
    "            \n",
    "            # Create embeddings for the batch\n",
    "            embeddings = model.encode(texts)\n",
    "            \n",
    "            # Prepare data for Pinecone\n",
    "            vector_data = []\n",
    "            for i, (chunk, embedding) in enumerate(zip(batch_chunks, embeddings)):\n",
    "                chunk_id = chunk.get('chunk_id', f'unknown-{start_idx + i}')\n",
    "                \n",
    "                # Prepare metadata (all columns except the vector itself)\n",
    "                metadata = {\n",
    "                    'source': chunk.get('source', ''),\n",
    "                    'text': chunk.get('text', ''),\n",
    "                    'timestamp': chunk.get('timestamp', ''),\n",
    "                    'title': chunk.get('title', '')\n",
    "                }\n",
    "                \n",
    "                # Add to vector data\n",
    "                vector_data.append({\n",
    "                    'id': chunk_id,\n",
    "                    'values': embedding.tolist(),\n",
    "                    'metadata': metadata\n",
    "                })\n",
    "            \n",
    "            # Upsert vectors to Pinecone\n",
    "            if vector_data:\n",
    "                index.upsert(vectors=vector_data)\n",
    "                logger.info(f\"Uploaded batch {batch_idx + 1}/{total_batches} to Pinecone\")\n",
    "            \n",
    "            # Small delay to avoid rate limiting\n",
    "            time.sleep(0.5)\n",
    "        \n",
    "        logger.info(f\"Successfully processed all {total_chunks} chunks to Pinecone\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing chunks to Pinecone: {e}\")\n",
    "        raise\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    try:\n",
    "        logger.info(\"Starting DynamoDB to Pinecone embedding process\")\n",
    "        \n",
    "        # Set up S3 logging if S3 URI is provided\n",
    "        s3_log_uri = os.environ.get(\"S3_LOG_URI\")\n",
    "        if s3_log_uri:\n",
    "            s3_handler = setup_s3_logging(s3_log_uri, interval=30)  # Upload logs every 30 seconds\n",
    "            # Make sure to flush logs to S3 at the end\n",
    "            if s3_handler:\n",
    "                logger.info(f\"Logs will be uploaded to {s3_log_uri}\")\n",
    "        \n",
    "        # Initialize services\n",
    "        dynamodb = init_dynamodb()\n",
    "        model = load_embedding_model()\n",
    "        index = init_pinecone()\n",
    "        \n",
    "        # Get all chunks from DynamoDB\n",
    "        chunks = get_all_chunks(dynamodb)\n",
    "        \n",
    "        # Process chunks, create embeddings, and upload to Pinecone\n",
    "        process_chunks_to_pinecone(chunks, model, index)\n",
    "        \n",
    "        # Get and display stats from Pinecone\n",
    "        stats = index.describe_stats()\n",
    "        logger.info(f\"Pinecone index stats: {stats}\")\n",
    "        \n",
    "        logger.info(\"Process completed successfully\")\n",
    "        \n",
    "        # Ensure logs are flushed to S3 before exiting\n",
    "        if s3_log_uri and 's3_handler' in locals() and s3_handler:\n",
    "            logger.info(\"Flushing final logs to S3...\")\n",
    "            s3_handler.flush()\n",
    "            logger.info(\"Logs successfully uploaded to S3\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Process failed with error: {e}\")\n",
    "        # Ensure logs are flushed to S3 in case of error\n",
    "        if 's3_handler' in locals() and s3_handler:\n",
    "            logger.info(\"Flushing error logs to S3...\")\n",
    "            s3_handler.flush()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
